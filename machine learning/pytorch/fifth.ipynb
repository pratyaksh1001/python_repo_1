{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_directml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu=torch_directml.default_device()\n",
    "gpu=torch_directml.device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\praty\\AppData\\Local\\Temp\\ipykernel_18260\\1776222190.py:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_train=torch.Tensor(np.array((pd.get_dummies(pd.read_csv(\"archive/Training_set.csv\")[\"label\"])).replace({True:1,False:0}))).to(gpu)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='privateuseone:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train=torch.Tensor(np.array((pd.get_dummies(pd.read_csv(\"archive/Training_set.csv\")[\"label\"])).replace({True:1,False:0}))).to(gpu)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=os.listdir(\"./archive/train/\")\n",
    "l=l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224, 224])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer=torchvision.transforms.Compose([torchvision.transforms.PILToTensor()])\n",
    "for i in range(len(l)):\n",
    "    img=PIL.Image.open(f\"./archive/train/{l[i]}\")\n",
    "    tensor=transformer(img)\n",
    "    l[i]=tensor\n",
    "l=torch.Tensor(np.array(l)).to(gpu)\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3,out_channels=32,kernel_size=2), # in=(3,224,224) out=(32,112,112)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32,out_channels=16,kernel_size=2,padding=1), # in=(16,112,112) out=(16,56,56)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=1,stride=1), # in=(16,56,56) out=(16,56,56)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((401408*2),512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,75),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layer1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ImageModel                               --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Conv2d: 2-1                       416\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Conv2d: 2-3                       2,064\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─MaxPool2d: 2-5                    --\n",
       "│    └─ReLU: 2-6                         --\n",
       "│    └─Flatten: 2-7                      --\n",
       "│    └─Linear: 2-8                       411,042,304\n",
       "│    └─ReLU: 2-9                         --\n",
       "│    └─Linear: 2-10                      38,475\n",
       "│    └─Sigmoid: 2-11                     --\n",
       "=================================================================\n",
       "Total params: 411,083,259\n",
       "Trainable params: 411,083,259\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=ImageModel().to(gpu)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(lr=0.01,params=model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 75])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1=l\n",
    "print(batch1.shape)\n",
    "batch1_y=y_train[:10].to(gpu)\n",
    "batch1_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.5397e-02, 3.7720e-04, 7.4046e-01, 7.1188e-01, 3.1827e-01, 9.9365e-01,\n",
      "         9.9453e-01, 2.3078e-03, 9.9803e-01, 8.4962e-01, 6.7458e-02, 4.9705e-01,\n",
      "         1.4146e-01, 9.9412e-01, 1.8873e-01, 9.8385e-01, 8.0719e-01, 9.7146e-01,\n",
      "         1.6358e-01, 6.4688e-03, 9.5346e-01, 9.4978e-01, 8.9031e-01, 8.2407e-02,\n",
      "         9.9469e-01, 6.5240e-01, 2.3018e-02, 9.1637e-01, 2.6946e-01, 9.9573e-01,\n",
      "         5.0686e-03, 1.3615e-02, 9.9831e-01, 4.5788e-01, 7.5330e-01, 9.9891e-01,\n",
      "         5.9662e-01, 4.3075e-01, 9.9851e-01, 1.0538e-03, 8.9694e-01, 1.3569e-03,\n",
      "         8.1040e-02, 5.1208e-01, 6.8113e-01, 7.7904e-01, 1.2408e-04, 9.5866e-01,\n",
      "         9.2396e-01, 2.6740e-02, 4.3957e-01, 2.7608e-01, 8.1829e-01, 3.2992e-01,\n",
      "         3.1828e-01, 1.1657e-01, 9.8409e-01, 5.5935e-02, 1.4783e-01, 2.1650e-01,\n",
      "         4.5730e-01, 2.5691e-02, 2.1965e-02, 4.2917e-02, 9.1013e-01, 9.7713e-01,\n",
      "         9.9875e-01, 7.4709e-03, 9.3321e-01, 6.7212e-01, 1.8582e-02, 7.2603e-04,\n",
      "         3.5030e-01, 8.1998e-02, 9.9816e-01],\n",
      "        [1.3587e-01, 1.9709e-03, 7.9835e-01, 6.6267e-01, 9.5848e-01, 9.9368e-01,\n",
      "         9.9741e-01, 3.3045e-01, 9.9685e-01, 7.4424e-01, 3.7017e-01, 5.0879e-01,\n",
      "         3.2261e-01, 7.9685e-01, 9.2363e-01, 9.9846e-01, 5.5724e-01, 9.7918e-01,\n",
      "         3.3989e-03, 2.6177e-03, 9.8610e-01, 8.2381e-01, 8.1562e-01, 5.2062e-02,\n",
      "         9.7981e-01, 9.2645e-01, 6.7825e-03, 9.6013e-01, 9.3008e-01, 9.4862e-01,\n",
      "         2.2159e-03, 1.1709e-01, 8.7944e-01, 4.8645e-01, 6.7977e-01, 9.9366e-01,\n",
      "         6.8709e-01, 7.9348e-01, 9.7495e-01, 1.7314e-04, 9.3177e-01, 5.0655e-04,\n",
      "         4.3959e-01, 4.4290e-01, 6.8766e-01, 9.6359e-01, 9.4668e-05, 7.9756e-01,\n",
      "         6.1821e-01, 2.2103e-01, 1.0968e-01, 1.1492e-01, 9.6033e-01, 2.6280e-01,\n",
      "         8.1358e-01, 5.4985e-02, 9.9783e-01, 2.8283e-02, 4.5607e-02, 4.0531e-01,\n",
      "         9.8515e-01, 4.3503e-02, 2.7622e-02, 3.9642e-01, 9.0740e-01, 9.0971e-01,\n",
      "         9.9358e-01, 1.0549e-02, 9.8213e-01, 6.3404e-01, 4.4842e-02, 8.8595e-03,\n",
      "         9.8115e-01, 5.3917e-02, 9.6707e-01],\n",
      "        [4.6281e-01, 5.5064e-04, 6.8214e-01, 9.8397e-01, 9.6786e-01, 5.8013e-01,\n",
      "         9.9959e-01, 8.5734e-01, 7.5187e-01, 8.5174e-01, 1.2714e-01, 6.0991e-01,\n",
      "         6.2542e-03, 9.4446e-01, 9.7250e-01, 9.9930e-01, 9.4842e-01, 9.9927e-01,\n",
      "         1.9766e-03, 1.6289e-03, 9.2086e-01, 9.8837e-01, 8.1833e-01, 1.4623e-02,\n",
      "         9.3641e-01, 8.1703e-01, 6.1115e-01, 9.5276e-01, 7.5215e-02, 2.2401e-01,\n",
      "         6.9917e-04, 2.2401e-02, 9.9832e-01, 9.7717e-01, 2.5614e-01, 9.9870e-01,\n",
      "         6.3730e-01, 5.0180e-02, 9.9985e-01, 6.8677e-04, 9.5498e-01, 5.6160e-04,\n",
      "         1.1471e-01, 8.8220e-01, 4.6444e-01, 9.3315e-01, 2.0195e-04, 1.2793e-01,\n",
      "         9.9492e-01, 1.1281e-01, 2.9961e-02, 5.7958e-02, 9.6022e-01, 1.7111e-02,\n",
      "         7.9689e-01, 7.2933e-01, 6.6488e-01, 1.4444e-02, 5.9003e-02, 7.4581e-01,\n",
      "         9.4362e-01, 7.6683e-03, 2.6725e-01, 3.5351e-02, 6.4395e-01, 9.7441e-01,\n",
      "         9.5815e-01, 6.7434e-02, 4.8476e-01, 9.9872e-01, 1.0476e-02, 7.7647e-03,\n",
      "         9.7328e-01, 5.2930e-03, 9.9397e-01],\n",
      "        [9.3217e-03, 1.9500e-04, 4.4432e-01, 2.4907e-01, 8.5402e-01, 9.9293e-01,\n",
      "         9.9879e-01, 1.0166e-01, 9.9989e-01, 9.5525e-01, 1.4297e-01, 5.3103e-01,\n",
      "         7.6938e-01, 3.3094e-01, 7.4719e-01, 9.9852e-01, 8.5145e-01, 9.5798e-01,\n",
      "         1.0626e-02, 5.2807e-04, 4.9951e-01, 9.7856e-01, 9.9364e-01, 4.8538e-02,\n",
      "         9.9572e-01, 8.7553e-01, 7.7738e-02, 9.2309e-01, 3.6997e-02, 9.6843e-01,\n",
      "         6.5492e-03, 5.1425e-02, 9.9981e-01, 8.3292e-01, 8.1977e-01, 9.7226e-01,\n",
      "         7.9939e-01, 2.1923e-02, 9.9894e-01, 1.1931e-05, 9.6400e-01, 4.7989e-05,\n",
      "         4.1208e-02, 8.9808e-01, 9.7550e-01, 2.3124e-02, 3.7659e-05, 8.9352e-01,\n",
      "         9.5813e-01, 3.3123e-02, 9.4762e-02, 1.8733e-01, 9.8645e-01, 4.5535e-02,\n",
      "         8.9032e-01, 7.2956e-02, 9.9960e-01, 9.6321e-02, 6.7609e-02, 1.9028e-01,\n",
      "         2.9081e-01, 2.3852e-03, 1.7850e-03, 8.6187e-02, 9.9063e-01, 9.7904e-01,\n",
      "         9.9983e-01, 7.2085e-03, 9.9253e-01, 9.9562e-01, 1.6436e-03, 4.0968e-04,\n",
      "         7.6336e-01, 9.5492e-03, 9.9995e-01],\n",
      "        [1.6238e-01, 8.1811e-02, 8.2467e-01, 5.8630e-01, 7.6363e-01, 9.9303e-01,\n",
      "         9.9822e-01, 5.4186e-01, 8.8302e-01, 8.7452e-01, 7.4395e-02, 4.4316e-01,\n",
      "         2.1568e-01, 8.5355e-01, 9.5901e-01, 9.7698e-01, 9.8742e-01, 9.9839e-01,\n",
      "         2.6065e-03, 1.5032e-03, 4.1828e-01, 9.9732e-01, 8.9432e-01, 9.5807e-01,\n",
      "         8.9410e-01, 7.2193e-01, 1.1658e-02, 8.3206e-01, 2.4204e-01, 9.8315e-01,\n",
      "         1.2931e-02, 5.6187e-03, 9.8869e-01, 6.9548e-01, 2.7033e-01, 9.7489e-01,\n",
      "         9.3142e-01, 7.3577e-01, 9.7366e-01, 7.9423e-03, 7.4585e-01, 1.2806e-02,\n",
      "         1.8897e-01, 9.1437e-01, 6.9826e-01, 8.7553e-01, 5.2348e-04, 5.2482e-01,\n",
      "         9.1363e-01, 7.5517e-02, 3.1177e-01, 2.0187e-01, 2.9234e-01, 8.4956e-01,\n",
      "         9.2707e-01, 6.7894e-01, 9.9800e-01, 2.4019e-01, 4.6555e-01, 7.8122e-02,\n",
      "         6.0702e-01, 5.0729e-02, 4.0262e-01, 7.9562e-01, 7.7932e-01, 9.7780e-01,\n",
      "         6.2689e-01, 1.2991e-02, 9.7029e-01, 9.8477e-01, 1.0614e-01, 1.2563e-02,\n",
      "         5.8827e-01, 5.6922e-02, 9.4990e-01],\n",
      "        [3.1127e-03, 3.8890e-03, 8.9758e-01, 3.1940e-01, 9.7392e-01, 9.9942e-01,\n",
      "         9.9997e-01, 3.6931e-02, 9.9536e-01, 9.5668e-01, 1.1104e-01, 1.8318e-01,\n",
      "         9.5911e-02, 9.4075e-01, 1.1167e-01, 9.9961e-01, 9.4391e-01, 9.9984e-01,\n",
      "         2.1112e-02, 7.0584e-04, 9.5514e-01, 9.2141e-01, 9.9345e-01, 1.9412e-01,\n",
      "         9.8054e-01, 6.7405e-01, 3.1440e-02, 7.8813e-01, 2.0844e-02, 9.9356e-01,\n",
      "         2.3088e-03, 2.2505e-02, 9.9113e-01, 1.2716e-01, 8.2730e-01, 9.9965e-01,\n",
      "         6.0064e-01, 2.9593e-01, 9.9871e-01, 5.4963e-05, 9.9560e-01, 1.3749e-06,\n",
      "         2.8878e-01, 9.7321e-01, 9.8804e-01, 6.9024e-01, 3.3384e-07, 9.5572e-01,\n",
      "         9.0138e-01, 1.7845e-01, 6.0135e-01, 7.5422e-02, 1.2288e-01, 6.8174e-01,\n",
      "         8.8409e-01, 8.6305e-03, 9.8300e-01, 9.4912e-03, 7.9579e-02, 1.4690e-02,\n",
      "         9.8828e-01, 2.0752e-01, 8.6691e-03, 1.0149e-01, 9.6690e-01, 8.9062e-01,\n",
      "         9.9990e-01, 8.9021e-03, 9.9725e-01, 9.9041e-01, 1.6784e-03, 5.6808e-05,\n",
      "         9.8948e-01, 7.0266e-03, 9.9948e-01],\n",
      "        [2.9630e-02, 1.5031e-01, 8.5348e-01, 5.6770e-01, 9.8070e-01, 9.9913e-01,\n",
      "         9.9970e-01, 9.3896e-01, 9.8920e-01, 9.5448e-01, 1.2930e-01, 9.8213e-01,\n",
      "         8.4925e-01, 3.9267e-01, 4.5527e-01, 9.9994e-01, 9.9478e-01, 9.9961e-01,\n",
      "         7.7602e-02, 3.2995e-03, 8.9964e-01, 9.9806e-01, 4.6644e-01, 4.2087e-01,\n",
      "         8.8470e-01, 3.1762e-01, 2.2402e-02, 9.8340e-01, 1.7940e-02, 9.9000e-01,\n",
      "         3.8777e-03, 5.4030e-03, 9.9024e-01, 7.6003e-01, 9.1616e-01, 9.9854e-01,\n",
      "         9.9991e-01, 7.2860e-01, 9.9885e-01, 6.9943e-04, 7.8105e-01, 1.8059e-03,\n",
      "         5.9706e-02, 9.7807e-01, 9.8558e-01, 6.1777e-02, 1.0208e-05, 9.0172e-01,\n",
      "         9.9850e-01, 1.0745e-02, 9.5034e-01, 3.5680e-02, 5.3671e-01, 2.2415e-01,\n",
      "         4.8459e-01, 3.9063e-01, 9.8858e-01, 2.5041e-03, 5.8768e-01, 3.2613e-02,\n",
      "         9.9304e-01, 2.4383e-02, 8.0853e-02, 9.5122e-01, 9.8093e-01, 9.9490e-01,\n",
      "         9.8728e-01, 1.3426e-02, 9.9960e-01, 9.3922e-01, 2.2034e-02, 2.0687e-04,\n",
      "         9.7840e-01, 3.9843e-02, 9.5786e-01],\n",
      "        [2.6507e-02, 1.4259e-03, 6.9895e-01, 2.7566e-01, 9.5120e-01, 9.9778e-01,\n",
      "         9.9870e-01, 3.6841e-01, 9.4279e-01, 9.0367e-01, 7.0772e-02, 5.3571e-01,\n",
      "         6.1779e-01, 6.9569e-01, 8.7337e-01, 9.9983e-01, 9.8445e-01, 9.8623e-01,\n",
      "         1.4857e-02, 1.4957e-04, 9.2948e-01, 9.9165e-01, 9.7515e-01, 1.1383e-01,\n",
      "         9.5438e-01, 9.4073e-01, 2.9583e-02, 9.5327e-01, 1.9942e-01, 9.7205e-01,\n",
      "         5.6259e-03, 1.4460e-02, 9.9610e-01, 3.2012e-01, 6.3898e-01, 9.8324e-01,\n",
      "         9.0478e-01, 7.9593e-01, 9.9833e-01, 2.7793e-04, 9.3642e-01, 1.8640e-04,\n",
      "         2.6815e-01, 9.5409e-01, 7.5817e-01, 2.7789e-01, 1.5149e-04, 7.2155e-01,\n",
      "         9.5394e-01, 1.2009e-03, 1.1687e-01, 1.5811e-02, 8.3086e-01, 6.1904e-01,\n",
      "         2.4975e-01, 2.1390e-01, 9.7282e-01, 3.1092e-02, 3.1741e-01, 6.5419e-02,\n",
      "         9.1992e-01, 5.6793e-02, 1.3792e-01, 5.5576e-02, 9.4543e-01, 9.8526e-01,\n",
      "         9.9754e-01, 1.2201e-03, 8.9360e-01, 9.7053e-01, 1.2332e-03, 7.3465e-03,\n",
      "         7.4663e-01, 2.6623e-01, 9.9154e-01],\n",
      "        [4.2910e-02, 1.1465e-04, 3.2099e-01, 8.5171e-01, 9.9160e-01, 8.6279e-01,\n",
      "         9.9812e-01, 5.6612e-02, 9.7609e-01, 8.3490e-01, 1.4098e-01, 7.6188e-02,\n",
      "         5.9874e-01, 9.9270e-01, 8.6191e-01, 9.9701e-01, 9.8979e-01, 9.9911e-01,\n",
      "         7.6815e-04, 2.1316e-05, 9.9798e-01, 9.7988e-01, 9.9331e-01, 2.1123e-01,\n",
      "         9.9935e-01, 7.5082e-01, 8.6266e-02, 8.7092e-01, 2.3312e-02, 5.7505e-01,\n",
      "         2.1435e-03, 1.2218e-03, 9.9895e-01, 7.1901e-02, 8.4901e-01, 9.6313e-01,\n",
      "         9.8131e-01, 8.7140e-02, 9.9998e-01, 3.8651e-05, 9.7643e-01, 4.5207e-05,\n",
      "         4.7662e-02, 7.7433e-01, 9.6372e-01, 5.1543e-01, 1.0293e-04, 9.6677e-01,\n",
      "         7.5230e-01, 3.9469e-02, 1.4308e-01, 2.9820e-01, 9.6335e-01, 5.2122e-04,\n",
      "         8.1172e-01, 1.1309e-02, 9.9790e-01, 7.2709e-03, 1.9179e-02, 1.8104e-01,\n",
      "         9.6092e-01, 4.6708e-02, 5.8514e-04, 8.2473e-02, 9.8169e-01, 9.2248e-01,\n",
      "         9.9994e-01, 1.4535e-02, 9.4658e-01, 9.6371e-01, 4.5660e-02, 1.8603e-03,\n",
      "         9.3919e-01, 1.9051e-03, 9.9884e-01],\n",
      "        [4.4824e-01, 1.8799e-02, 9.8991e-01, 1.8379e-02, 8.1305e-01, 9.8614e-01,\n",
      "         9.9636e-01, 2.4627e-01, 9.4428e-01, 8.1205e-01, 3.9067e-01, 2.8359e-01,\n",
      "         8.4691e-02, 6.3201e-01, 9.5963e-01, 9.8724e-01, 8.5631e-01, 9.9184e-01,\n",
      "         3.3284e-01, 3.0567e-03, 8.8252e-01, 8.7438e-01, 8.2044e-01, 8.0741e-01,\n",
      "         3.9539e-01, 1.7105e-01, 9.6803e-02, 7.7954e-01, 6.1851e-01, 9.6833e-01,\n",
      "         1.9831e-03, 2.7722e-02, 9.6603e-01, 5.8193e-01, 8.5390e-01, 9.6335e-01,\n",
      "         7.1456e-01, 3.5477e-01, 8.7323e-01, 1.0076e-02, 7.4735e-01, 6.5736e-04,\n",
      "         7.0365e-01, 9.5195e-01, 8.2561e-01, 9.6134e-03, 5.3851e-03, 4.9514e-01,\n",
      "         6.3985e-01, 3.0929e-02, 7.6421e-01, 3.6712e-02, 3.4544e-01, 5.1490e-01,\n",
      "         7.5953e-01, 7.3676e-01, 8.9820e-01, 7.1113e-02, 5.7515e-01, 8.7796e-02,\n",
      "         7.9893e-01, 1.0760e-01, 1.1467e-01, 1.3022e-01, 8.8030e-01, 9.5248e-01,\n",
      "         9.9459e-01, 4.8597e-03, 8.3105e-01, 9.2171e-01, 6.3100e-02, 3.4207e-02,\n",
      "         7.1308e-01, 5.6544e-01, 9.5818e-01]], device='privateuseone:0',\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not allocate tensor with 1644167168 bytes. There is not enough GPU video memory available!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss_func(y_preds,batch1_y)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not allocate tensor with 1644167168 bytes. There is not enough GPU video memory available!"
     ]
    }
   ],
   "source": [
    "epochs=1\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    y_preds=model.forward(batch1)\n",
    "    print(y_preds)\n",
    "    loss=loss_func(y_preds,batch1_y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The parameter is incorrect.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m\n\u001b[1;32m----> 3\u001b[0m y_res\u001b[38;5;241m=\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_res\u001b[38;5;241m.\u001b[39margmax())\n",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m, in \u001b[0;36mImageModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_jit_internal.py:503\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\praty\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The parameter is incorrect."
     ]
    }
   ],
   "source": [
    "x=1234\n",
    "\n",
    "y_res=(model.forward(l[x:x+1]).cpu().detach().numpy())\n",
    "print(y_res.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(58, device='privateuseone:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[x:x+1].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
